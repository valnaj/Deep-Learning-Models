{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "cuDNN is available\n",
      "Device is set to: cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# see if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "\n",
    "# see if cuDNN is available\n",
    "if torch.backends.cudnn.enabled:\n",
    "    print(\"cuDNN is available\")\n",
    "\n",
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is set to:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at: ./data\\shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Define the location and file name for the dataset\n",
    "dataset_file_name = 'shakespeare.txt'\n",
    "dataset_file_origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "dataset_dir = './data'\n",
    "dataset_file_path = os.path.join(dataset_dir, dataset_file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.isfile(dataset_file_path):\n",
    "    urllib.request.urlretrieve(dataset_file_origin, dataset_file_path)\n",
    "    print(f\"Downloaded: {dataset_file_path}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at: {dataset_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(dataset_file_path, mode='r').read()\n",
    "\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "print('vocab:', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Map characters to their indices in vocabulary.\n",
    "char2index = {char: index for index, char in enumerate(vocab)}\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(char2index, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2index[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# Map character indices to characters from vacabulary.\n",
    "index2char = np.array(vocab)\n",
    "print(index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_as_int length: 1115394\n",
      "'First Citizen:\\n' --> array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])\n"
     ]
    }
   ],
   "source": [
    "# Convert chars in text to indices.\n",
    "text_as_int = np.array([char2index[char] for char in text])\n",
    "\n",
    "print('text_as_int length: {}'.format(len(text_as_int)))\n",
    "print('{} --> {}'.format(repr(text[:15]), repr(text_as_int[:15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[43, 56, 10,  ...,  6,  1, 41],\n",
      "        [49,  6,  1,  ..., 58,  1, 58],\n",
      "        [ 1, 46, 43,  ..., 43, 10,  0],\n",
      "        ...,\n",
      "        [ 0,  5, 32,  ..., 23, 17,  1],\n",
      "        [ 1, 39, 52,  ..., 35, 46, 39],\n",
      "        [57, 57, 43,  ..., 52, 42, 50]], device='cuda:0')\n",
      "Target: tensor([[56, 10,  1,  ...,  1, 41, 53],\n",
      "        [ 6,  1, 47,  ...,  1, 58, 46],\n",
      "        [46, 43, 56,  ..., 10,  0, 32],\n",
      "        ...,\n",
      "        [ 5, 32, 47,  ..., 17,  1, 27],\n",
      "        [39, 52, 42,  ..., 46, 39, 58],\n",
      "        [57, 43, 42,  ..., 42, 50, 43]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import tensorboard\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "# Create the Dataset\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, sequence_length):\n",
    "        self.text = text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_seq = torch.tensor(self.text[index:index+self.sequence_length], device=device, dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.text[index+1:index+self.sequence_length+1], device=device, dtype=torch.long)\n",
    "        return (input_seq, target_seq)\n",
    "\n",
    "sequence_length = 100 \n",
    "dataset = ShakespeareDataset(text_as_int, sequence_length)\n",
    "\n",
    "# DataLoader for handling batching\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "# Instantiate the data module\n",
    "data_module = DataModule(dataset)\n",
    "\n",
    "# You can access the DataLoader like this:\n",
    "train_loader = data_module.train_dataloader()\n",
    "\n",
    "# To check the output\n",
    "for batch in train_loader:\n",
    "    input_text, target_text = batch\n",
    "    print('Input:', input_text)\n",
    "    print('Target:', target_text)\n",
    "    break  # Only print the first batch to check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationLSTM(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "        super(TextGenerationLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=rnn_units, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        self.reset_hidden_state()\n",
    "        loss = nn.functional.cross_entropy(y_hat.transpose(1, 2), y)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Owner\\Documents\\MMA\\Assignment 2 RNN\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory C:\\Users\\Owner\\Documents\\MMA\\Assignment 2 RNN\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 16.6 K\n",
      "1 | lstm      | LSTM      | 5.3 M \n",
      "2 | fc        | Linear    | 66.6 K\n",
      "----------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.337    Total estimated model params size (MB)\n",
      "c:\\Users\\Owner\\Documents\\MMA\\Assignment 2 RNN\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 17426/17426 [06:33<00:00, 44.28it/s, v_num=19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 17426/17426 [06:34<00:00, 44.15it/s, v_num=19]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = TextGenerationLSTM(vocab_size=len(vocab), embedding_dim=256, rnn_units=1024, batch_size=64)\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='model-{epoch:02d}-{train_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"text_generation_lstm\")\n",
    "\n",
    "# Instantiate the PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    devices=1, \n",
    "    accelerator='gpu',\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path: C:\\Users\\Owner\\Documents\\MMA\\Assignment 2 RNN\\checkpoints\\model-epoch=17-train_loss=0.15.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextGenerationLSTM(\n",
       "  (embedding): Embedding(65, 256)\n",
       "  (lstm): LSTM(256, 1024, batch_first=True)\n",
       "  (fc): Linear(in_features=1024, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(\"Best model path:\", best_model_path)\n",
    "\n",
    "# Restore the best checkpoint\n",
    "model = TextGenerationLSTM.load_from_checkpoint(\n",
    "    checkpoint_path=best_model_path,\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=256,\n",
    "    rnn_units=1024,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: alas!\n",
      "\n",
      "CLAUDIO:\n",
      "Sweet said, and here and stands avoidings: you were contrary\n",
      "\n",
      "POMPEY:\n",
      "I beseech you, sir, let him go with gentle-skeeping part;\n",
      "And here I take thee affection, rise and stop our further long,\n",
      "Thou shouldst bear momentime that we have almost\n",
      "Most great with wrongs in being known to give.\n",
      "\n",
      "GLOUCESTER:\n",
      "Look, how this ring encompasseth finger.\n",
      "Even so thy breast encloseth my poor children.\n",
      "If thou wilt outward forth from our service done:\n",
      "My brother Angelo hath lost a honour'd none,\n",
      "And yet no further than a wanton's bird;\n",
      "Who lets it hop a little from her hand,\n",
      "Like a poor prisoner in his twisted gyves,\n",
      "And with a silk thread plucks it back again,\n",
      "So loving-jealous of his liberty.\n",
      "\n",
      "ROMEO:\n",
      "I would I were thy bird.\n",
      "\n",
      "JULIET:\n",
      "Sweet, so would I:\n",
      "Yet I should kill thee with much cherishing.\n",
      "Good night, good night! parting is such\n",
      "sweet land amiss to come hither: Thou\n",
      "Must, both my adventure in your own pricy.\n",
      "\n",
      "BAPTISTA:\n",
      "You are welcome all.\n",
      "\n",
      "PETRUCHIO:\n",
      "She hath prevented me. Her\n",
      "JULIET: O God'st Die! 'tis so much,\n",
      "While I mine an office, or sake, scenting tears nor coals\n",
      "Here are the kense of the deep, or any fair Bianca\n",
      "Till the part, which was young, save us you, Camillo;\n",
      "I think it well: you are in arms;\n",
      "I confess the day: O, prayers, home-brother,\n",
      "Nor how to she dangerous and grusts and till\n",
      "My blood fully oath; in first to dip is to\n",
      "Coriolanusly fate, the King of Nupinague!\n",
      "Deserves this sword in joy, cried 'God sell nor good,\n",
      "To show the sound of your younger daughter,\n",
      "Then my avoid, cambless on the bier!\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "And many an obbedicutio stolent.\n",
      "\n",
      "JORN ELIZABETH:\n",
      "Of that there have I namely to\n",
      "counterfeitions, and I love my cousin\n",
      "Upon his body that did refuse him, he would\n",
      "Have made them melest, and hanst outness shall die,\n",
      "Take you a cursely gave away'?\n",
      "Ah, what a scord of the catest holding to it.\n",
      "\n",
      "HERMIONE:\n",
      "Come alive, I'll keep my talk; therefore be rose\n",
      "In those you will our good counsel:\n",
      "Honour and respect, alread, brether come in presence\n",
      "\n",
      "NOR\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_indices = torch.tensor([char2index[s] for s in start_string]).unsqueeze(0).to(model.device)\n",
    "    text_generated = []\n",
    "    model.reset_hidden_state()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_indices)\n",
    "            predictions = predictions[:, -1, :] / temperature\n",
    "            probabilities = nn.functional.softmax(predictions, dim=-1).squeeze().cpu().numpy()\n",
    "            predicted_id = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "            input_indices = torch.tensor([[predicted_id]]).to(model.device)\n",
    "            text_generated.append(index2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate text\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n",
    "\n",
    "# Generate text with higher temperature\n",
    "print(generate_text(model, start_string=\"JULIET: \", temperature=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING EDWARD IV:\n",
      "Now here a period of tumultuous broils.\n",
      "Away with Oxford to Hames Castle straight:\n",
      "For Somerset, off with his guilty hand:\n",
      "Madam 'ay 'em, and much better blood I begin: I pare\n",
      "Before thy bond shall be such severe past\n",
      "cure of the thing you wot of, unless they kept very\n",
      "good diet, as I told you,--\n",
      "\n",
      "FROTH:\n",
      "All this is true.\n",
      "\n",
      "POMPEY:\n",
      "Why, very well, then,--\n",
      "\n",
      "ESCALUS:\n",
      "Come, you are a tedious fool: to the purpose. What\n",
      "was done to Elbow's wife, that he hath cause to\n",
      "complain of? Come me to what was done to her.\n",
      "\n",
      "POMPEY:\n",
      "Sir, your honour cannot come to that yet.\n",
      "\n",
      "ESCALUS:\n",
      "No, sir, nor I mean it not.\n",
      "\n",
      "POMPEY:\n",
      "Sir, but you shall come again to Mantua.\n",
      "And this shall free thee from this present shame;\n",
      "If no inconstant toy, nor woman's flesh.\n",
      "\n",
      "BAPTISTA:\n",
      "It was your find this man in holy wedlock bands.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Yes, I agree, and thank he's ever.\n",
      "\n",
      "LUCENTIO:\n",
      "Faith, sir, if you had told as many lies in his\n",
      "behalf as you have uttered words in your own, you\n",
      "should not pass here; no,\n",
      "What 'twere to call it?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Honder an ass'-naked; capbar father, Ariel, saw her lamenurse,\n",
      "And say he not?\n",
      "\n",
      "BENVOLIO:\n",
      "Wh't will forget'st. Thou art not certain?\n",
      "Is not thy wit were with't! Thou hast not swear't. To know\n",
      "That's tweney one--God fell you, will you get me dear\n",
      "ABlory!\n",
      "\n",
      "All:\n",
      "Conuetine: delieve not as high as heaven itself is transh'd.\n",
      "\n",
      "Provost:\n",
      "A Paulina,\n",
      "Was my great Warwick and the defence:\n",
      "Upon shepress'd that enders 'em for Bohemia?\n",
      "\n",
      "Mapery:\n",
      "Tell him when his sighs fiery lay hole France,\n",
      "Ratch truch withhe tyily remedy.\n",
      "But, Cliffince, learned is waters: in the air,\n",
      "That near not our coffer earthly.\n",
      "3 KING HENRY VI\n",
      "\n",
      "First Keeper:\n",
      "Under this thick-right to Cybot and always late\n",
      "To calf with weed; and will disperse the propertaint;\n",
      "And, by God, sir, to owe away;\n",
      "They know, it valions and severainties,\n",
      "Yet papt up of child, will rin'd the right.\n",
      "Keepen, Hear me, give me worself to hear:\n",
      "He is not half our title of thy throat,\n",
      "So I did yield to do't:\n",
      "Julless defend \n"
     ]
    }
   ],
   "source": [
    "# Other example\n",
    "\n",
    "print(generate_text(model, start_string=\"KING\"))\n",
    "\n",
    "print(generate_text(model, start_string=\"What \", temperature=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
